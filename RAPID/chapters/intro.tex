\chapter{Introduction}
\label{intro}

\section{Problem definition}
\label{Problem}
Several hundred thousand miles of energy pipelines span the whole of North America---responsible for carrying the natural gas and liquid petroleum that power the continent's homes and economies~\cite{PHMSA}. These pipelines, so crucial to everyday goings-on, are closely monitored by various operating companies to ensure they perform safely and smoothly~\cite{PHMSA2013}.

Happenings like earthquakes, erosion, and extreme weather---and human factors like vehicle traffic and construction---all pose threats to pipeline integrity~\cite{MichaelBakerJr.2008,Chastain,Dunning2013}. Compromised lines put business activities at risk; individual safety, personal property, and the surrounding environment feel the impacts as well~\cite{Dunning2013}. However, with appropriate knowledge and management processes, these issues can be avoided~\cite{Dunning2013,Chastain}.

As such, there is a tremendous need to measure and indicate useful, actionable data for each region of interest. It is difficult, though, to keep a close-enough eye on massive pipeline networks with manual, in-person inspections~\cite{Dunning2013,Chastain}. Even recurring aerial surveillance demands too much information from too few (and distant) sources~\cite{Dunning2013}.

As an alternative to visiting pipeline sites, operators often use computer-based decision support systems (DSS) to learn about and allocate resources for active and potential hazards~\cite{PHMSA2013,Dunning2013}. 

In general, these DSS allow operators to coalesce and study geographic, demographic, and remote sensor data relevant to pipeline integrity management~\cite{RedlandsSDSS,Dunning2013}. A great deal of worthwhile DSS data is already recorded, stored, and made available for analysis, but it is scattered across physical and digital media and locations. As such, pipeline operators are left to pick and choose smaller subsets of attainable and manageable inputs for analysis, which can end up painting an incomplete picture~\cite{Dunning2013}.

\section{Contributions}
We have created specific system requirements alongside web developers and industry partners to address their primary data aggregation and analysis needs. The remainder of this document describes the implemented, deployed, and validated software for improving the amount and quality of data available to pipeline operator DSS, which we dubbed \textit{REST API for Pipeline Integrity Data} (RAPID). More specifically, RAPID allows third-party applications to manage and query an arbitrary number of disparate geographic data sources through one REST API. Rather than leaving DSS and their users to identify, model, convert between, and store dozens of data formats and repositories, our system manages and hides that trivia---instead exposing simple REST calls for more consistency and centralization.

A lot of geospatial data is conceptually alike because it pairs a physical location or demarcation with information about that place. The crux of the problem from Section~\ref{Problem}, though, is that there are numerous and varied formats and access methods for those geospatial datasets. However, with appropriate modeling and processing, they can each be stored, modified, and queried in approximately the same way.

As we built it, RAPID uses the Python-based Django web framework with a PostgreSQL database. To store and query first-class geospatial data, PostgreSQL's PostGIS extension has been added and enabled, and our schema stores geometries and metadata located on Earth's surface.

Our most significant contribution to this project was the design and implementation of RAPID's data model (along with supplementary tools). In this document, we describe the process and peculiarities of building the RAPID model and query interface for real-world pipeline integrity management.

While the database itself is not accessible to external users, developers and applications can interact with the aggregated content through a well-defined REST API.\footnote{REST, or \textit{Representational State Transfer} is an architectural style for web service APIs, often using HTTP~\cite{Francis}.} The API provides means of looking up geospatial data for particular regions (in space and time), as well as necessary metadata and documentation for its use and interpretation.

As a crude example, this allows API users to make a request like ``retrieve all rain data for California in JSON'' as well as a request like ``retrieve all \textit{seismic} data for California in JSON \textit{from last week}.'' The rain data may have originated as JSON files from portable weather stations; the seismic data could have come from a Shapefile on a government server.\footnote{We discuss particulars for RAPID's supported formats in the following chapters.} The point is that customers can view and download different forms of data from wildly different sources in a mostly-standard and interchangeable fashion.

My research partner, Alexa Francis, focuses on the REST API in her master's thesis, covering its implementation and usage, with considerations for third-party developers and our internal system architecture~\cite{Francis}.

\label{polyview_intro}
Finally, serving to validate our system, an external web application has been built to use RAPID---both retrieving and visualizing geospatial data. This additional software, RAPID UI, aggregates and indexes several datasets relevant to our pipeline operating partners and displays \textit{a}) how they are organized within our system and \textit{b}) where and what the geospatial features are.

RAPID UI looks up important locations and associated properties using the RAPID API, letting the UI perform quick and easy location-based queries with only HTTP requests---normally requiring custom-built logic for spatial processing. We have also used RAPID UI's activities to guide performance measurements and optimization. While our own test data does not strain RAPID as much as a pipeline-covered continent would, we also show system efficiency and stability during real, on-demand work.

\paragraph{Document outline.}
Following this chapter, Chapter~\ref{background} discusses the most essential and helpful context for creating RAPID, including related software, standards, and theory. Chapter~\ref{requirements} overviews our guiding set of system requirements---coming from partners and commonplace industry recommendations. Chapter~\ref{design} shows how we built RAPID, designing the data model and algorithms in Python. Chapter~\ref{validation} describes our procedure for validating RAPID, showing a correct and efficient implemented solution. Chapter~\ref{conclusions} concludes this document, summarizing our work and other contributions that may follow.


% \subsection{Data Management Dashboard}
% Lastly, a web dashboard for data management was created for customers, which lets them add, update, and remove tracked geospatial features. When someone points the dashboard at an input dataset, it is cleaned, parsed, and added to the integrated database. The user interface and application functionality are robust and friendly enough to handle the most common input formats, and they allow for intuitive viewing and modification of previously-added data.