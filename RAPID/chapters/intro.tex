\chapter{Introduction}
\label{intro}

\section{Problem Definition}
\label{Problem}
Several hundred thousand miles of energy pipelines span the whole of North America---responsible for carrying the natural gas and liquid petroleum that literally power the continent's homes and economies~\cite{PHMSA}. These pipelines, so crucial to everyday goings-on, are closely monitored by various operating companies to ensure they perform safely and smoothly~\cite{PHMSA2013}.

Happenings like earthquakes, erosion, and extreme weather---and human factors like vehicle traffic and construction---all pose threats to pipeline integrity. Not only can compromised pipelines delay business activities, personal property and safety may be at risk, and protecting the surrounding environment from negative ecological impacts is of similar concern. However, with appropriate knowledge of and preparedness for these situations, many issues can be avoided entirely.

As such, there's a tremendous need to measure and indicate useful, actionable data for each region of interest. It's difficult, though, to keep a close-enough eye on massive pipeline networks with manual, in-person inspections. Even frequent aerial surveillance demands too much information from too few (and distant) sources.

Alternative to visiting pipeline sites, operators often use computer-based decision support systems (DSSs) to learn and make determinations about active and potential hazards~\cite{PHMSA2013,Dunning2013}. 

Most simply, these DSSs allow operators to coalesce and study geographic, demographic, and remote sensor data relevant to pipeline integrity management. A whole lot of worthwhile DSS data is already recorded, stored, and made available for analysis, but it's scattered across physical and digital formats and locations~\cite{Dunning2013}. As such, pipeline operators are left to pick and choose smaller subsets of attainable and manageable inputs for analysis, which can end up painting an incomplete picture.

\section{Contributions}
The remainder of this document describes an implemented, deployed, and validated software system for improving the amount and quality of data available to pipeline operator DSSs. The project is named \textit{REST Access for Pipeline Integrity Data} (RAPID).

More specifically, RAPID allows user applications to manage and query an arbitrary number of disparate geospatial data sources through one REST API. Rather than leaving DSSs and their users to identify, model, convert between, and store dozens of data formats and repositories, our system takes responsibility for and hides that trivia, instead exposing simple REST calls for more consistency and centralization.

\subsection{Geospatial Data Model}
A lot of geospatial data is conceptually alike because it pairs a physical location or demarcation with information about that place. The crux of the problem described in Section~\ref{Problem} is that there are numerous and varied formats and access methods for those geospatial data sets, but with appropriate modeling and processing, they can each be stored, modified, and queried in approximately the same way.

RAPID is built on a PostgreSQL database with the PostGIS geospatial extension added and enabled. Very basically, the schema stores geometries and quantities located on Earth's surface as well as relevant metadata. The design and implementation of this data model was my significant contribution to RAPID, and this thesis covers the process and peculiarities of building a model for real-world pipeline integrity management.

Related work that also makes up this contribution 

\subsection{REST API}
While the database itself will not be directly accessible to external users, developers and applications can interact with the aggregated content through a well-defined REST API. The API provides means of looking up geospatial data for particular regions, as well as necessary metadata for its use and interpretation.

As a crude example, this allows API users to make a request like ``retrieve all rain data for California in JSON'' as well as a request like ``retrieve all \textit{seismic} data for California in JSON.'' The rain data may have originated as CSVs from portable weather stations; the seismic data could have come from a Shapefile on a government file server. The point is that customers can view and download wildly different forms of data from wildly different sources in a mostly-standard and interchangeable fashion.

\subsection{Data Management Dashboard}
Lastly, a web dashboard for data management was created for customers, which lets them add, update, and remove tracked geospatial features. When someone points the dashboard at an input data set, it's cleaned, parsed, and added to the integrated database. The user interface and application functionality are robust and friendly enough to handle the most common input formats, and they allow for intuitive viewing and modification of previously-added data.