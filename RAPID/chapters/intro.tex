\chapter{Introduction}
\label{intro}

\section{Problem definition}
\label{Problem}
Several hundred thousand miles of energy pipelines span the whole of North America---responsible for carrying the natural gas and liquid petroleum that power the continent's homes and economies~\cite{PHMSA}. These pipelines, so crucial to everyday goings-on, are closely monitored by various operating companies to ensure they perform safely and smoothly~\cite{PHMSA2013}.

Happenings like earthquakes, erosion, and extreme weather---and human factors like vehicle traffic and construction---all pose threats to pipeline integrity~\cite{MichaelBakerJr.2008,Chastain,Dunning2013}. Compromised lines put business activities at risk; individual safety, personal property, and the surrounding environment feel the impacts as well~\cite{Dunning2013}. However, with appropriate knowledge and management processes, these issues can be avoided~\cite{Dunning2013,Chastain}.

As such, there's a tremendous need to measure and indicate useful, actionable data for each region of interest. It's difficult, though, to keep a close-enough eye on massive pipeline networks with manual, in-person inspections~\cite{Dunning2013,Chastain}. Even recurring aerial surveillance demands too much information from too few (and distant) sources~\cite{Dunning2013}.

Alternative to visiting pipeline sites, operators often use computer-based decision support systems (DSS) to learn and allocate resources for active and potential hazards~\cite{PHMSA2013,Dunning2013}. 

In general, these DSS allow operators to coalesce and study geographic, demographic, and remote sensor data relevant to pipeline integrity management~\cite{RedlandsSDSS,Dunning2013}. A great deal of worthwhile DSS data is already recorded, stored, and made available for analysis, but it's scattered across physical and digital media and locations. As such, pipeline operators are left to pick and choose smaller subsets of attainable and manageable inputs for analysis, which can end up painting an incomplete picture~\cite{Dunning2013}.

\section{Contributions}
We've created specific system requirements alongside web developers and industry partners to address their primary data aggregation and analysis needs. The remainder of this document describes the implemented, deployed, and validated software for improving the amount and quality of data available to pipeline operator DSS, which we've dubbed \textit{REST API for Pipeline Integrity Data} (RAPID). More specifically, RAPID allows third-party applications to manage and query an arbitrary number of disparate geographic data sources through one REST API. Rather than leaving DSS and their users to identify, model, convert between, and store dozens of data formats and repositories, our system manages and hides that trivia---instead exposing simple REST calls for more consistency and centralization.

A lot of geospatial data is conceptually alike because it pairs a physical location or demarcation with information about that place. The crux of the problem from Section~\ref{Problem}, though, is that there are numerous and varied formats and access methods for those geospatial data sets. However, with appropriate modeling and processing, they can each be stored, modified, and queried in approximately the same way.

As we built it, RAPID uses the Python-based Django web framework with a PostgreSQL database. To store and query first-class geospatial data, PostgreSQL's PostGIS extension has been added and enabled, and our schema stores geometries and metadata located on Earth's surface.

Our most significant contribution to this project was the design and implementation of RAPID's data model (along with supplementary tools). In this document, we describe the process and peculiarities of building the RAPID model and query interface for real-world pipeline integrity management.

While the database itself is not accessible to external users, developers and applications can interact with the aggregated content through a well-defined REST API. The API provides means of looking up geospatial data for particular regions (in space and time), as well as necessary metadata and documentation for its use and interpretation.

As a crude example, this allows API users to make a request like ``retrieve all rain data for California in JSON'' as well as a request like ``retrieve all \textit{seismic} data for California in JSON \textit{from last week}.'' The rain data may have originated as JSON files from portable weather stations; the seismic data could have come from a Shapefile on a government server.\footnote{We discuss particulars for RAPID's supported formats in the following chapters.} The point is that customers can view and download different forms of data from wildly different sources in a mostly-standard and interchangeable fashion.

My research partner, Alexa Francis, focuses on the REST API in her master's thesis, covering its implementation and usage, with considerations for third-party developers and our internal system architecture~\cite{Francis}.

\label{polyview_intro}
Finally, serving to validate our system, an external web application has been modified to use RAPID---both adding and retrieving geospatial data. This additional software, PolyView, aggregates and indexes several datasets relevant to Cal Poly, including class schedules and the campus map.

PolyView utilizes the RAPID API to store building locations and associated properties. This enables PolyView to later perform quick and easy location-based queries with only HTTP requests---normally requiring custom-built logic for spatial processing or integration with other third-party libraries. We've also used PolyView as a testbed for performance measurements and optimization. While PolyView data doesn't strain RAPID as much as pipeline-covered continent would, its usage gives a sense of system efficiency and stability during real, on-demand work.

% \subsection{Data Management Dashboard}
% Lastly, a web dashboard for data management was created for customers, which lets them add, update, and remove tracked geospatial features. When someone points the dashboard at an input data set, it's cleaned, parsed, and added to the integrated database. The user interface and application functionality are robust and friendly enough to handle the most common input formats, and they allow for intuitive viewing and modification of previously-added data.