\chapter{Verification and validation}

\section{Introduction}
RAPID includes a geospatial data model as well as file parsing and querying capabilities. We must, therefore, (1) verify that RAPID's design meets it specification and (2) validate that our code base parses, converts, calculates, and outputs geospatial data \textit{correctly}.

\section{Feature verification}
Whether or not RAPID is completely bug-free, its design and feature set still must be verified against system requirements. This is particularly prudent, as part of its entire purpose is to enable portability and interchangeability for external partners. RAPID can be verified empirically with the following guidelines.

There is a set of agreed-upon requirements for RAPID. These include the supported data formats, necessary data model considerations, and known querying features. These requirements can be summarized into short statements such as ``GeoJSON files can be added to the system'' or ``Atmospheric data can be retrieved for any region.'' Valid responses rank from 1 to 5---1 being ``Totally disagree'' and 5 being ``Totally agree.''

A set of requirements (maybe fifteen of the central features) can be turned into the concise statements and presented to one of three  participant types, a creator, a user, or a third-party, along with access to RAPID and its documentation. RAPID's creators (us), should test the system from our perspective to ensure that we believe we've implemented the functionality as originally planned. Users, or customers, will provide good insight into whether they felt \textit{their} needs were met. Another third-party software developer or GIS expert can act as a tie-breaker of sorts, if there's too much uncertainty.

With each of the above participant types represented sufficiently, participants consistently agreeing that core functionality is present would be the best indicator for a verified system. Mismatched ratings between groups may indicate a fundamental misunderstanding or misinterpretation in one conceptual area.

With participants tested in the form above, their process for reaching an answer on the 1-5 scale can be evaluated for further information. A reasonable hypothesis is ``RAPID fulfills its requirements.'' Numbers below a certain threshold, along with qualitative measurements, can disprove that hypothesis. Several feasible measurements include the following:

\begin{itemize}
\item The resources consulted when arriving at an answer. What did or didn't participants find that influenced their decision?
\item Background knowledge used to make the decision. Was the participant referencing facts that they knew previously? How did they come to know that?
\item Influence of opinion. Did the participant ever (even previously) express a particular interest in one feature or another that went well or poorly?
\item Technical flaws found. During this testing process, any technical problems should be logged and investigated (see the next section for further discussion of possibilities in this area).
\end{itemize}

All of this criteria can be weighed and considered to determine how closely RAPID meets its specified requirements.

\section{Unit testing}
One of RAPID's main technical contributions is its data- and file-handling abilities, being able to parse, convert between, and output several structured geospatial formats. The formats usually have similar fields (with data that's highly-specific to established geospatial concepts), so it's mostly trivial to check that the same data is represented in multiple file formats.

RAPID, additionally, does much of this in a repeatable fashion---reading, writing, and abstractly storing several common file and data types with modular code.

All of these components can be validated with a thorough unit test suite. On the back end, RAPID uses the Django web framework, which utilizes Python's standard library testing module, \texttt{unittest}~\cite{DjangoTesting}. Once each necessary data field is accounted for in each possible input and output format, exhaustive unit tests can ensure that each field in one format can be converted to the corresponding field in every other format.\footnote{A more efficient methodology would be to focus on common fields converted between common formats.}

Unit testing is also important because it validates the technical aspects of the data model (even though it can be verified conceptually in other ways\footnote{Like ontological modeling.}). Geospatial data in RAPID's scope can be found in one of several forms:

\begin{itemize}
\item Writable and parseable digital format like GeoJSON or Shapefile.
\item In-memory objects.
\item Well-known text (WKT)---OGC's abstract text format for geospatial features.
\item Abstract data added to and retrieved from PostGIS.
\end{itemize}

Data can move linearly up and down that list, but can not skip from the top to the bottom or vice versa. Each of those steps and transitions can also be viewed as a data integrity checkpoint that deserves a unit test. For instance, a polygon made up of bordering points (0, 0), (0, 10), and (10, 0) still needs to have the same meaning---a triangle with those three points---no matter where it's located. A GeoJSON file, a Python object, the WKT notation, and PostGIS queries and data structures can all be inspected to ensure that the same data is contained in each. If it isn't, the data was misshaped somewhere along the line that can, hopefully, be traced.

\section{PolyView}
\label{polyview_details}
\label{validation_bbox}


% NOTES
% Differences between on-the-fly file conversion and exporting abstract data
% Using a bounding box